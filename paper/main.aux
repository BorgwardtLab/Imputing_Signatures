\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\new@tpo@label[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\BKM@entry[2]{}
\BKM@entry{id=1,dest={73656374696F6E2E31},srcline={120}}{496E74726F64756374696F6E}
\citation{Chen54,Chen57,Chen58}
\citation{lyons1998differential,FritzVictoir10,lyons2014rough}
\citation{kidger2019deep}
\citation{hambly2010uniqueness}
\citation{lyons1998differential}
\citation{reyna2019early,morrill2019signature}
\citation{levin2013,kidger2019deep,fermanian2019embedding}
\citation{iisignature,signatory}
\citation{kidger2019deep,levin2013}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\citation{kidger2019deep}
\citation{rubin1976inference}
\citation{li2016scalable,futoma2017mgp}
\BKM@entry{id=2,dest={73656374696F6E2E32},srcline={176}}{52656C6174656420776F726B}
\citation{primer2016,kormilitzlin2016,yang2016rotation,li2017lpsnet,yang2017leveraging,PerezArribas2018,morrill2019sepsis}
\citation{jeremythesis}
\citation{logsigrnn}
\citation{kidger2019deep}
\citation{chevyrev2018signature,kiraly2019kernels}
\citation{toth2019gp}
\citation{primer2016,fermanian2019embedding}
\citation{levin2013}
\citation{gelman2007dataanalysis}
\citation{li2016scalable}
\citation{shukla2018interpolationprediction}
\citation{rubanova2019latent,kidger2020neuralcde}
\citation{che2018recurrent}
\citation{horn2019set}
\BKM@entry{id=3,dest={73656374696F6E2E33},srcline={216}}{4261636B67726F756E643A205369676E6174757265207472616E73666F726D20616E6420476175737369616E2070726F63657373206164617074657273}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related work}{2}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Imputation schemes}{2}{section*.1}\protected@file@percent }
\citation{primer2016}
\citation{kidger2019deep}
\citation{signatory}
\citation{li2016scalable,futoma2017mgp}
\@writefile{toc}{\contentsline {section}{\numberline {3}Background: Signature transform and Gaussian process adapters}{3}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Path signatures}{3}{section*.2}\protected@file@percent }
\newlabel{eq:signature}{{1}{3}{Path signatures}{equation.3.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Given a path (bold), its Levy area is its signed area with respect to the chord joining its endpoints.\relax }}{3}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:sig_path}{{1}{3}{Given a path (bold), its Levy area is its signed area with respect to the chord joining its endpoints.\relax }{figure.caption.3}{}}
\newlabel{sec: computing sig}{{3}{3}{Computing the signature transform}{section*.4}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Computing the signature transform}{3}{section*.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Notation}{3}{section*.5}\protected@file@percent }
\newlabel{eq:seriesspace}{{2}{3}{Notation}{equation.3.2}{}}
\newlabel{sec: GPadapter}{{3}{3}{Gaussian process adapter}{section*.6}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Gaussian process adapter}{3}{section*.6}\protected@file@percent }
\BKM@entry{id=4,dest={73656374696F6E2E34},srcline={317}}{5061746820696D7075746174696F6E7320666F72207369676E6174757265206D6F64656C73}
\citation{levin2013}
\citation{kidger2019deep}
\citation{li2016scalable}
\citation{futoma2017mgp}
\citation{li2016scalable}
\citation{futoma2017mgp}
\newlabel{eq:gp-mc}{{3}{4}{Gaussian process adapter}{equation.3.3}{}}
\newlabel{eq:gp-mean}{{6}{4}{Gaussian process adapter}{equation.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Path imputations for signature models}{4}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Task}{4}{section*.7}\protected@file@percent }
\newlabel{eq:objective}{{7}{4}{Task}{equation.4.7}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Path imputation strategies}{4}{section*.8}\protected@file@percent }
\citation{lyons1998differential}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Overview of our proposed extension of GP adapters, GP-PoM, leveraging both posterior moments\nobreakspace  {}(mean and variance). In comparison, the conventional GP adapter feeds MC samples\nobreakspace  {}(faded colours in the background) drawn from the GP posterior into the classifier. \relax }}{5}{figure.caption.9}\protected@file@percent }
\newlabel{fig:overview}{{2}{5}{Overview of our proposed extension of GP adapters, GP-PoM, leveraging both posterior moments~(mean and variance). In comparison, the conventional GP adapter feeds MC samples~(faded colours in the background) drawn from the GP posterior into the classifier. \relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline GP adapter with posterior moments}{5}{section*.10}\protected@file@percent }
\newlabel{eq:gp-moments}{{10}{5}{GP adapter with posterior moments}{equation.4.10}{}}
\BKM@entry{id=5,dest={73656374696F6E2E35},srcline={441}}{4578706572696D656E7473}
\citation{goldberger2000physiobank}
\citation{Dua2019}
\citation{allam2018photometric}
\citation{Dua2019}
\citation{cho2014learning}
\citation{kidger2019deep}
\citation{signatory}
\citation{gardner2018gpytorch}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experiments}{6}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Datasets and preprocessing}{6}{section*.11}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Models}{6}{section*.12}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Training and evaluation}{6}{section*.13}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \texttt  {CharacterTrajectories} dataset under label-based subsampling. The top three methods are highlighted: bold \& underlined, bold, underlined. All measures are reported as percentage points. Balanced accuracy (BAC) is the metric we optimised for. We further report accuracy and weighted AUROC (w-AUROC).\relax }}{7}{table.caption.14}\protected@file@percent }
\newlabel{tab:character}{{1}{7}{\texttt {CharacterTrajectories} dataset under label-based subsampling. The top three methods are highlighted: bold \& underlined, bold, underlined. All measures are reported as percentage points. Balanced accuracy (BAC) is the metric we optimised for. We further report accuracy and weighted AUROC (w-AUROC).\relax }{table.caption.14}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \texttt  {PenDigits} dataset under label-based subsampling. The top three methods are highlighted: bold \& underlined, bold, underlined. All measures are reported as percentage points. Balanced accuracy (BAC) is the metric we optimised for. We further report accuracy and weighted AUROC (w-AUROC)\relax }}{8}{table.caption.15}\protected@file@percent }
\newlabel{tab:pendigits}{{2}{8}{\texttt {PenDigits} dataset under label-based subsampling. The top three methods are highlighted: bold \& underlined, bold, underlined. All measures are reported as percentage points. Balanced accuracy (BAC) is the metric we optimised for. We further report accuracy and weighted AUROC (w-AUROC)\relax }{table.caption.15}{}}
\BKM@entry{id=6,dest={73656374696F6E2E36},srcline={583}}{44697363757373696F6E}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Experimental results visualised for \texttt  {CharacterTrajectories} dataset. The bars display performance in terms of balanced accuracy (BAC), whereas the panels indicate the subsampling strategy. Left: Random subsampling (R), right: label-based subsampling (L).\relax }}{9}{figure.caption.17}\protected@file@percent }
\newlabel{fig:results_main}{{3}{9}{Experimental results visualised for \texttt {CharacterTrajectories} dataset. The bars display performance in terms of balanced accuracy (BAC), whereas the panels indicate the subsampling strategy. Left: Random subsampling (R), right: label-based subsampling (L).\relax }{figure.caption.17}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Results}{9}{figure.caption.17}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{9}{section.6}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Recommendations for the practitioner}{9}{section*.18}\protected@file@percent }
\BKM@entry{id=7,dest={73656374696F6E2E37},srcline={616}}{436F6E636C7573696F6E}
\citation{fey2018splinecnn}
\citation{FUNAHASHI1993801,continuousrnn}
\BKM@entry{id=8,dest={73656374696F6E2E38},srcline={644}}{41636B6E6F776C656467656D656E7473}
\bibdata{ref}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{10}{section.7}\protected@file@percent }
\bibcite{allam2018photometric}{{1}{2018}{{Allam~Jr et~al.}}{{}}}
\bibcite{continuousrnn}{{2}{1998}{{Bailer-Jones et~al.}}{{}}}
\bibcite{kidger2019deep}{{3}{2019}{{Bonnier et~al.}}{{}}}
\bibcite{che2018recurrent}{{4}{2018}{{Che et~al.}}{{}}}
\bibcite{Chen54}{{5}{1954}{{Chen}}{{}}}
\bibcite{Chen57}{{6}{1957}{{Chen}}{{}}}
\bibcite{Chen58}{{7}{1958}{{Chen}}{{}}}
\bibcite{primer2016}{{8}{2016}{{Chevyrev and Kormilitzin}}{{}}}
\bibcite{chevyrev2018signature}{{9}{2018}{{Chevyrev and Oberhauser}}{{}}}
\bibcite{cho2014learning}{{10}{2014}{{Cho et~al.}}{{}}}
\bibcite{Dua2019}{{11}{2017}{{Dua and Graff}}{{}}}
\bibcite{fermanian2019embedding}{{12}{2019}{{Fermanian}}{{}}}
\bibcite{fey2018splinecnn}{{13}{2018}{{Fey et~al.}}{{}}}
\bibcite{FritzVictoir10}{{14}{2010}{{Friz and Victoir}}{{}}}
\bibcite{FUNAHASHI1993801}{{15}{1993}{{Funahashi and Nakamura}}{{}}}
\bibcite{futoma2017mgp}{{16}{2017}{{Futoma et~al.}}{{}}}
\bibcite{gardner2018gpytorch}{{17}{2018}{{Gardner et~al.}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Acknowledgements}{11}{section.8}\protected@file@percent }
\bibcite{gelman2007dataanalysis}{{18}{2007}{{Gelman and Hill}}{{}}}
\bibcite{goldberger2000physiobank}{{19}{2000}{{Goldberger et~al.}}{{}}}
\bibcite{hambly2010uniqueness}{{20}{2010}{{{Hambly} and {Lyons}}}{{}}}
\bibcite{horn2019set}{{21}{2019}{{Horn et~al.}}{{}}}
\bibcite{signatory}{{22}{2020}{{Kidger and Lyons}}{{}}}
\bibcite{kidger2020neuralcde}{{23}{2020}{{Kidger et~al.}}{{}}}
\bibcite{kingma2014adam}{{24}{2014}{{Kingma and Ba}}{{}}}
\bibcite{kiraly2019kernels}{{25}{2019}{{Kir{\'a}ly and Oberhauser}}{{}}}
\bibcite{kormilitzlin2016}{{26}{2016}{{Kormilitzin et~al.}}{{}}}
\bibcite{levin2013}{{27}{2013}{{Levin et~al.}}{{}}}
\bibcite{li2017lpsnet}{{28}{2017}{{Li et~al.}}{{}}}
\bibcite{li2016scalable}{{29}{2016}{{Li and Marlin}}{{}}}
\bibcite{logsigrnn}{{30}{2019}{{Liao et~al.}}{{}}}
\bibcite{lyons2014rough}{{31}{2014}{{Lyons}}{{}}}
\bibcite{lyons1998differential}{{32}{1998}{{Lyons}}{{}}}
\bibcite{moor2019early}{{33}{2019}{{Moor et~al.}}{{}}}
\bibcite{morrill2019signature}{{34}{2019a}{{Morrill et~al.}}{{}}}
\bibcite{morrill2019sepsis}{{35}{2019b}{{Morrill et~al.}}{{}}}
\bibcite{pytorch2019}{{36}{2019}{{Paszke et~al.}}{{}}}
\bibcite{PerezArribas2018}{{37}{2018}{{Perez~Arribas et~al.}}{{}}}
\bibcite{jeremythesis}{{38}{2019}{{Reizenstein}}{{}}}
\bibcite{iisignature}{{39}{2018}{{Reizenstein and Graham}}{{}}}
\bibcite{reyna2019early}{{40}{2019}{{Reyna et~al.}}{{}}}
\bibcite{rubanova2019latent}{{41}{2019}{{Rubanova et~al.}}{{}}}
\bibcite{rubin1976inference}{{42}{1976}{{Rubin}}{{}}}
\bibcite{shukla2018interpolationprediction}{{43}{2019}{{Shukla and Marlin}}{{}}}
\bibcite{toth2019gp}{{44}{2019}{{Toth and Oberhauser}}{{}}}
\bibcite{yang2016rotation}{{45}{2016}{{Yang et~al.}}{{}}}
\bibcite{yang2017leveraging}{{46}{2017}{{Yang et~al.}}{{}}}
\bibstyle{apalike}
\BKM@entry{id=9,dest={617070656E6469782E41},srcline={654}}{417070656E646978}
\BKM@entry{id=10,dest={73756273656374696F6E2E412E31},srcline={656}}{46757274686572204578706572696D656E7473}
\BKM@entry{id=11,dest={73756273656374696F6E2E412E32},srcline={751}}{496D7075746174696F6E2073747261746567696573}
\@writefile{toc}{\contentsline {section}{\numberline {A}Appendix}{14}{appendix.A}\protected@file@percent }
\newlabel{sec:Appendix}{{A}{14}{Appendix}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Further Experiments}{14}{subsection.A.1}\protected@file@percent }
\newlabel{supp:Experiments}{{A.1}{14}{Further Experiments}{subsection.A.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \texttt  {CharacterTrajectories}, random subsampling\relax }}{14}{table.caption.21}\protected@file@percent }
\newlabel{supp: tab character}{{3}{14}{\texttt {CharacterTrajectories}, random subsampling\relax }{table.caption.21}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Imputation strategies}{14}{subsection.A.2}\protected@file@percent }
\newlabel{supp: Imputation}{{A.2}{14}{Imputation strategies}{subsection.A.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \texttt  {PenDigits}, random subsampling\relax }}{15}{table.caption.22}\protected@file@percent }
\newlabel{supp: tab pendigits}{{4}{15}{\texttt {PenDigits}, random subsampling\relax }{table.caption.22}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces \texttt  {LSST}, label-based subsampling\relax }}{16}{table.caption.23}\protected@file@percent }
\newlabel{supp: tab lsst label-based}{{5}{16}{\texttt {LSST}, label-based subsampling\relax }{table.caption.23}{}}
\@writefile{lot}{\contentsline {table}{\numberline {6}{\ignorespaces \texttt  {LSST}, random subsampling\relax }}{17}{table.caption.24}\protected@file@percent }
\newlabel{supp: tab lsst random}{{6}{17}{\texttt {LSST}, random subsampling\relax }{table.caption.24}{}}
\@writefile{lot}{\contentsline {table}{\numberline {7}{\ignorespaces \texttt  {Physionet 2012} \relax }}{18}{table.caption.25}\protected@file@percent }
\newlabel{supp: tab physionet}{{7}{18}{\texttt {Physionet 2012} \relax }{table.caption.25}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Visualisations for \texttt  {CharacterTrajectories} and \texttt  {LSST} . The rows indicate datasets and different subsampling schemes (R for Random, L for Label-based). The left column displays the performance metric which was optimzied for: balanced accuracy (BAC), or average precision. The right column indicates the number of trainable parameters which the best model required (as selected in the hyperparameter search). \relax }}{19}{figure.caption.26}\protected@file@percent }
\newlabel{supp: barplots1}{{4}{19}{Visualisations for \texttt {CharacterTrajectories} and \texttt {LSST} . The rows indicate datasets and different subsampling schemes (R for Random, L for Label-based). The left column displays the performance metric which was optimzied for: balanced accuracy (BAC), or average precision. The right column indicates the number of trainable parameters which the best model required (as selected in the hyperparameter search). \relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Visualisations for \texttt  {PenDigits} and \texttt  {Physionet} . The rows indicate datasets and different subsampling schemes (R for Random, L for Label-based). The left column displays the performance metric which was optimzied for: balanced accuracy (BAC), or average precision. The right column indicates the number of trainable parameters which the best model required (as selected in the hyperparameter search). \relax }}{20}{figure.caption.27}\protected@file@percent }
\newlabel{supp: barplots2}{{5}{20}{Visualisations for \texttt {PenDigits} and \texttt {Physionet} . The rows indicate datasets and different subsampling schemes (R for Random, L for Label-based). The left column displays the performance metric which was optimzied for: balanced accuracy (BAC), or average precision. The right column indicates the number of trainable parameters which the best model required (as selected in the hyperparameter search). \relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces GP-PoM training illustrated for CharacterTrajectories as compared to conventional GP adapter.\relax }}{20}{figure.caption.28}\protected@file@percent }
\newlabel{supp: gp-training}{{6}{20}{GP-PoM training illustrated for CharacterTrajectories as compared to conventional GP adapter.\relax }{figure.caption.28}{}}
\BKM@entry{id=12,dest={73756273656374696F6E2E412E33},srcline={771}}{44617461736574207374617469737469637320616E642066696C746572696E67}
\citation{goldberger2000physiobank}
\citation{Dua2019}
\citation{allam2018photometric}
\citation{Dua2019}
\BKM@entry{id=13,dest={73756273656374696F6E2E412E34},srcline={787}}{4D6F64656C20696D706C656D656E746174696F6E732C206172636869746563747572657320616E64206879706572706172616D6574657273}
\citation{pytorch2019}
\citation{gardner2018gpytorch}
\citation{kidger2019deep}
\citation{kidger2019deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Dataset statistics and filtering}{21}{subsection.A.3}\protected@file@percent }
\newlabel{supp: Dataset stats}{{A.3}{21}{Dataset statistics and filtering}{subsection.A.3}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline Physionet2012}{21}{section*.29}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline PenDigits}{21}{section*.30}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline LSST}{21}{section*.31}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline CharacterTrajectories}{21}{section*.32}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Model implementations, architectures and hyperparameters}{21}{subsection.A.4}\protected@file@percent }
\newlabel{supp: Model Architectures}{{A.4}{21}{Model implementations, architectures and hyperparameters}{subsection.A.4}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline \textsc  {Sig}}{21}{section*.33}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline \textsc  {RNNSig}}{21}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{\nonumberline \textsc  {RNN}}{21}{section*.35}\protected@file@percent }
\citation{kingma2014adam}
\citation{futoma2017mgp,moor2019early}
\BKM@entry{id=14,dest={73756273656374696F6E2E412E35},srcline={820}}{46726167696C6520646570656E64656E6365206F6E2073616D706C696E6720696E20756E72656C61746564206368616E6E656C733A206578616D706C65}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  L{\'e}vy area of the forward-fill imputed path. By changing $t_{3/2}$\nobreakspace  {}(a \emph  {single} unrelated observation!), we can make this disparity greater or smaller. \relax }}{22}{figure.caption.37}\protected@file@percent }
\newlabel{fig:bentline}{{7}{22}{L{\'e}vy area of the forward-fill imputed path. By changing $t_{3/2}$~(a \emph {single} unrelated observation!), we can make this disparity greater or smaller. \relax }{figure.caption.37}{}}
\@writefile{toc}{\contentsline {paragraph}{\nonumberline \textsc  {DeepSig}}{22}{section*.36}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Fragile dependence on sampling in unrelated channels: example}{22}{subsection.A.5}\protected@file@percent }
\newlabel{sec:Fragile dependence}{{A.5}{22}{Fragile dependence on sampling in unrelated channels: example}{subsection.A.5}{}}
\newlabel{eq:flaw1}{{11}{22}{Fragile dependence on sampling in unrelated channels: example}{equation.A.11}{}}
\newlabel{eq:flaw2}{{12}{22}{Fragile dependence on sampling in unrelated channels: example}{equation.A.12}{}}
\BKM@entry{id=15,dest={73756273656374696F6E2E412E36},srcline={899}}{43617573616C207369676E617475726520696D7075746174696F6E}
\citation{kidger2019deep}
\citation{toth2019gp}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.6}Causal signature imputation}{23}{subsection.A.6}\protected@file@percent }
\newlabel{sec:Causal signature imputation}{{A.6}{23}{Causal signature imputation}{subsection.A.6}{}}
\newlabel{eq:causalsig}{{14}{23}{Causal signature imputation}{equation.A.14}{}}
\citation{kidger2019deep}
\citation{primer2016}
\citation{fermanian2019embedding}
\citation{levin2013}
\newlabel{theorem:invariancetime}{{1}{24}{Invariance to reparameterisation}{theorem.1}{}}
\newlabel{eq:causal1}{{15}{24}{Causal signature imputation}{equation.A.15}{}}
\newlabel{eq:causal2}{{16}{24}{Causal signature imputation}{equation.A.16}{}}
\newlabel{sec: comparison fourier}{{A.6}{25}{Causal signature imputation}{equation.A.16}{}}
\global\csname @altsecnumformattrue\endcsname
