{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.datasets import Physionet2012, dict_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = Physionet2012(split='training')\n",
    "dataset_loader = DataLoader(dataset, batch_size=16, collate_fn=dict_collate_fn,\n",
    "                        shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataset_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['time', 'values', 'label'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.keys() # dict of following keys containing each a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 125, 36])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor shape: [bs, distinct times, channels]\n",
    "batch['values'].shape #contains nans for invalid points (no measurement at given time and channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([125, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# time steps of the first instance of batch #for shorter instance (eg. instance 1) this is \n",
    "# padded with zeros for tensor compatibility\n",
    "\n",
    "batch['time'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 1)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['time'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'time': array([[ 0.       ],\n",
       "        [ 0.25     ],\n",
       "        [ 1.4166666],\n",
       "        [ 2.0333333],\n",
       "        [ 2.4166667],\n",
       "        [ 2.8666666],\n",
       "        [ 3.0333333],\n",
       "        [ 3.8666666],\n",
       "        [ 3.9      ],\n",
       "        [ 3.9833333],\n",
       "        [ 4.0666666],\n",
       "        [ 4.15     ],\n",
       "        [ 4.233333 ],\n",
       "        [ 4.3166666],\n",
       "        [ 4.4      ],\n",
       "        [ 4.4666667],\n",
       "        [ 4.483333 ],\n",
       "        [ 4.5666666],\n",
       "        [ 4.65     ],\n",
       "        [ 4.733333 ],\n",
       "        [ 4.8166666],\n",
       "        [ 4.9      ],\n",
       "        [ 4.983333 ],\n",
       "        [ 5.0666666],\n",
       "        [ 5.15     ],\n",
       "        [ 5.233333 ],\n",
       "        [ 5.25     ],\n",
       "        [ 5.4      ],\n",
       "        [ 5.4666667],\n",
       "        [ 6.4      ],\n",
       "        [ 6.4666667],\n",
       "        [ 6.65     ],\n",
       "        [ 6.9      ],\n",
       "        [ 7.15     ],\n",
       "        [ 7.1666665],\n",
       "        [ 7.4      ],\n",
       "        [ 8.166667 ],\n",
       "        [ 8.4      ],\n",
       "        [ 8.9      ],\n",
       "        [ 8.983334 ],\n",
       "        [ 9.066667 ],\n",
       "        [ 9.15     ],\n",
       "        [ 9.4      ],\n",
       "        [ 9.883333 ],\n",
       "        [ 9.9      ],\n",
       "        [10.4      ],\n",
       "        [10.683333 ],\n",
       "        [10.883333 ],\n",
       "        [10.9      ],\n",
       "        [11.4      ],\n",
       "        [11.65     ],\n",
       "        [11.733334 ],\n",
       "        [12.066667 ],\n",
       "        [12.4      ],\n",
       "        [12.65     ],\n",
       "        [13.4      ],\n",
       "        [14.4      ],\n",
       "        [14.9      ],\n",
       "        [15.4      ],\n",
       "        [15.65     ],\n",
       "        [15.9      ],\n",
       "        [15.95     ],\n",
       "        [16.15     ],\n",
       "        [16.4      ],\n",
       "        [16.65     ],\n",
       "        [16.816668 ],\n",
       "        [16.95     ],\n",
       "        [17.4      ],\n",
       "        [17.45     ],\n",
       "        [18.4      ],\n",
       "        [19.4      ],\n",
       "        [20.4      ],\n",
       "        [21.266666 ],\n",
       "        [21.4      ],\n",
       "        [22.4      ],\n",
       "        [23.266666 ],\n",
       "        [23.4      ],\n",
       "        [24.083334 ],\n",
       "        [24.266666 ],\n",
       "        [24.4      ],\n",
       "        [25.4      ],\n",
       "        [26.4      ],\n",
       "        [26.85     ],\n",
       "        [27.4      ],\n",
       "        [27.5      ],\n",
       "        [27.85     ],\n",
       "        [28.4      ],\n",
       "        [28.666666 ],\n",
       "        [29.4      ],\n",
       "        [30.4      ],\n",
       "        [30.9      ],\n",
       "        [31.4      ],\n",
       "        [31.433332 ],\n",
       "        [32.4      ],\n",
       "        [33.05     ],\n",
       "        [33.4      ],\n",
       "        [33.483334 ],\n",
       "        [33.616665 ],\n",
       "        [33.65     ],\n",
       "        [34.05     ],\n",
       "        [34.4      ],\n",
       "        [35.066666 ],\n",
       "        [35.4      ],\n",
       "        [35.9      ],\n",
       "        [36.4      ],\n",
       "        [36.9      ],\n",
       "        [37.15     ],\n",
       "        [37.233334 ],\n",
       "        [37.25     ],\n",
       "        [37.4      ],\n",
       "        [38.383335 ],\n",
       "        [38.4      ],\n",
       "        [39.2      ],\n",
       "        [39.383335 ],\n",
       "        [39.4      ],\n",
       "        [40.4      ],\n",
       "        [41.4      ],\n",
       "        [42.4      ],\n",
       "        [42.65     ],\n",
       "        [43.4      ],\n",
       "        [44.4      ],\n",
       "        [45.4      ],\n",
       "        [46.4      ],\n",
       "        [46.45     ],\n",
       "        [47.4      ]], dtype=float32),\n",
       " 'values': array([[        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "          0.06764641,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan, -0.03613645],\n",
       "        ...,\n",
       "        [        nan,         nan,         nan, ...,  0.00408607,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ...,         nan,\n",
       "                 nan,         nan],\n",
       "        [        nan,         nan,         nan, ..., -0.22210568,\n",
       "                 nan,         nan]], dtype=float32),\n",
       " 'label': array([0.], dtype=float32)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports for UEA dataset class:\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.datasets import Dataset\n",
    "from src.datasets.utils import DATA_DIR\n",
    "import uea_ucr_datasets #requires the data in ~/.data/UEA_UCR\n",
    "\n",
    "# from ..tasks import BinaryClassification\n",
    "from src.datasets.mimic_benchmarks_utils import Normalizer\n",
    "from src.datasets.utils import DATA_DIR\n",
    "\n",
    "DATASET_BASE_PATH = os.path.join(DATA_DIR, 'UEA_UCR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "uea_ucr_datasets.list_datasets()\n",
    "dataset = 'PenDigits'  #'PhonemeSpectra' #'SpokenArabicDigits' #'UWaveGestureLibrary'\n",
    "d = uea_ucr_datasets.Dataset(dataset, train=True)\n",
    "first_instance = d[0]\n",
    "instance_x, instance_y = first_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []; y = []\n",
    "for i, (instance_x, instance_y) in enumerate(d):\n",
    "    X.append(instance_x)\n",
    "    y.append(instance_y)\n",
    "X = np.array(X); y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array([[ 47., 100.],\n",
       "       [ 27.,  81.]], dtype=float32),\n",
       "       array([[  0.,  89.],\n",
       "       [ 27., 100.],\n",
       "       [ 42.,  75.],\n",
       "       [ 29.,  45.],\n",
       "       [ 15.,  15.],\n",
       "       [ 37.,   0.],\n",
       "       [ 69.,   2.],\n",
       "       [100.,   6.]], dtype=float32),\n",
       "       array([[  0.,  57.],\n",
       "       [ 31.,  68.],\n",
       "       [ 72.,  90.],\n",
       "       [100., 100.],\n",
       "       [ 76.,  75.],\n",
       "       [ 50.,  51.],\n",
       "       [ 28.,  25.],\n",
       "       [ 16.,   0.]], dtype=float32),\n",
       "       ...,\n",
       "       array([[100.,  98.],\n",
       "       [ 60., 100.],\n",
       "       [ 24.,  87.],\n",
       "       [  3.,  58.],\n",
       "       [ 35.,  51.],\n",
       "       [ 58.,  26.],\n",
       "       [ 36.,   0.],\n",
       "       [  0.,   5.]], dtype=float32),\n",
       "       array([[ 59.,  65.],\n",
       "       [ 91., 100.],\n",
       "       [ 84.,  96.],\n",
       "       [ 72.,  50.],\n",
       "       [ 51.,   8.],\n",
       "       [  0.,   0.],\n",
       "       [ 45.,   1.],\n",
       "       [100.,   0.]], dtype=float32),\n",
       "       array([[  0.,  78.],\n",
       "       [ 29., 100.],\n",
       "       [ 94.,  86.],\n",
       "       [ 70.,  48.],\n",
       "       [ 42.,  11.],\n",
       "       [ 32.,   0.],\n",
       "       [ 25.,  36.],\n",
       "       [100.,  40.]], dtype=float32)], dtype=object)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "sss.get_n_splits(Z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, test_index = next(sss.split(Z, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3700, 4987,  892, ..., 5871, 5874, 1057])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z = np.arange(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 7491, 7492, 7493])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([3700, 4987,  892, ..., 5871, 5874, 1057]),\n",
       " array([1442, 2190, 3894, ..., 4154, 2999, 2051]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_index, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([156., 156., 156., 144., 156., 144., 144., 155., 144., 144.]),\n",
       " array([0. , 0.9, 1.8, 2.7, 3.6, 4.5, 5.4, 6.3, 7.2, 8.1, 9. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y[test_index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-90d0c9689649>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports for UEA dataset class:\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "from src.datasets import Dataset\n",
    "from src.datasets.utils import DATA_DIR\n",
    "import uea_ucr_datasets #requires the data in ~/.data/UEA_UCR\n",
    "\n",
    "# from ..tasks import BinaryClassification\n",
    "from src.datasets.mimic_benchmarks_utils import Normalizer\n",
    "from src.datasets.utils import DATA_DIR\n",
    "\n",
    "DATASET_BASE_PATH = os.path.join(DATA_DIR, 'UEA')\n",
    "\n",
    "class UEADataReader():\n",
    "    \"\"\"UEA Data Reader to read and return instances of given split.\"\"\"\n",
    "    def __init__(self, dataset_name, split, out_path):\n",
    "        uea_ucr_datasets.list_datasets()\n",
    "        if split == 'testing':\n",
    "            data = uea_ucr_datasets.Dataset(dataset_name, train=False)\n",
    "            self.X, self.y = _to_array(data)\n",
    "        elif split in ['training', 'validation']:\n",
    "            validation_split_file = os.path.join(out_path, 'validation_split_file.pkl')\n",
    "            data = uea_ucr_datasets.Dataset(dataset_name, train=True)\n",
    "            X, y = _to_array(data)\n",
    "            if not os.path.isfile(validation_split_file):\n",
    "                print('Generating stratified training/validation split...')\n",
    "                #now create the splits:\n",
    "                from sklearn.model_selection import StratifiedShuffleSplit\n",
    "                sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "                #X_dummy = np.zeros([len(y),2])\n",
    "                sss.get_n_splits(X, y) #for simpler NaN handling, we use dummy data for splitting, \n",
    "                #as only labels are relevant\n",
    "                \n",
    "                training_indices, validation_indices = next(sss.split(X, y))\n",
    "                split_dict = {'training': training_indices,\n",
    "                              'validation': validation_indices\n",
    "                }\n",
    "                #save the split ids\n",
    "                if not os.path.exists(out_path):\n",
    "                    os.makedirs(out_path, exist_ok=True)\n",
    "                with open(validation_split_file, 'wb') as f:\n",
    "                    pickle.dump(split_dict, f )#protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            else:\n",
    "                print('Loading stratified training/validation split.')\n",
    "                with open(validation_split_file, 'rb') as f:\n",
    "                    split_dict = pickle.load(f)\n",
    "            indices = split_dict[split]\n",
    "            self.X = X[indices] #subsetting the split\n",
    "            self.y =  y[indices] \n",
    "        else:\n",
    "            raise ValueError('Provided split not available.', \n",
    "                             'Use any of [training, validation, testing]'\n",
    "                            )\n",
    "    def read_example(self, index):\n",
    "        return {'X': self.X[index], 'y': self.y[index]}\n",
    "    def get_number_of_examples(self):\n",
    "        return len(self.X)\n",
    "        \n",
    "        \n",
    "class UEADataset(Dataset):\n",
    "    \"\"\"UEA Dataset Class to load any UEA dataset.\"\"\"\n",
    "\n",
    "    #Here, the normalizer config path is still abstract, to be formatted later:\n",
    "    normalizer_config = os.path.join(\n",
    "        #os.path.dirname(__file__), #hard code for now as not possible to use it here\n",
    "        '/Users/mimoor/Desktop/localwork/Signatures/GP_Signatures/src/datasets',\n",
    "        'resources',\n",
    "        '{}Dataset_normalization.json'\n",
    "    )\n",
    "    \n",
    "    def __init__(self, dataset_name, split, transform=None, data_path=DATASET_BASE_PATH):\n",
    "        \"\"\"Initialize dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset_name: Name of UEA dataset to load.\n",
    "                [ PenDigits, .. ] \n",
    "            split: Name of split. One of `training`, `validation`, `testing`.\n",
    "            data_path: Path to data. Default:\n",
    "                {project_root_dir}/data/UEA\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_path = os.path.join(data_path, dataset_name)\n",
    "\n",
    "        #self.dataset = uea_ucr_datasets.Dataset(dataset_name, train=True)\n",
    "        self.reader = UEADataReader(dataset_name, split, self.data_path)\n",
    "        self.normalizer_config = self.normalizer_config.format(dataset_name)\n",
    "        self.normalizer = Normalizer()\n",
    "        self._set_properties()        \n",
    "        \n",
    "        if not os.path.exists(self.normalizer_config):\n",
    "            print(f'Normalizer config {self.normalizer_config} not found!')\n",
    "            print('Generating normalizer config...')\n",
    "            if split != 'training':\n",
    "                # Only allow to compute normalization statics on training split\n",
    "                raise ValueError(\n",
    "                    'Not allowed to compute normalization data '\n",
    "                    'on other splits than training.'\n",
    "                )\n",
    "            for i in trange(len(self)):\n",
    "                instance = self.reader.read_example(i)['X']\n",
    "                self.normalizer._feed_data(instance)\n",
    "            self.normalizer._save_params(self.normalizer_config)\n",
    "        else:\n",
    "            self.normalizer.load_params(self.normalizer_config)\n",
    "\n",
    "        self.maybe_transform = transform if transform else lambda a: a\n",
    "    \n",
    "    def _set_properties(self):\n",
    "        self.has_unaligned_measurements = False\n",
    "        self.statics = None\n",
    "        self.n_statics = 0\n",
    "        instance = self.reader.read_example(0)\n",
    "        self._measurement_dims = instance['X'].shape[1]\n",
    "\n",
    "    @property\n",
    "    def n_classes(self):\n",
    "        \"\"\"\n",
    "        Refering to multi-label settings (conforming with other pipelines)\n",
    "        \"\"\"\n",
    "        return 1 \n",
    "    \n",
    "    @property\n",
    "    def n_class_types(self):\n",
    "        \"\"\"\n",
    "        Given single-label settings, how many classes / class manifestations of this label\n",
    "        \"\"\"\n",
    "        distinct_labels = np.unique(self.reader.y)\n",
    "        return len(distinct_labels)\n",
    "\n",
    "    @property\n",
    "    def measurement_dims(self):\n",
    "        return self._measurement_dims\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.reader.get_number_of_examples()\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        instance = self.reader.read_example(index)\n",
    "        features = self.normalizer.transform(instance['X'])\n",
    "        label = instance['y']\n",
    "        if self.n_classes == 1:\n",
    "            # Add an additional dimension to the label if it is only a scalar.\n",
    "            # This makes it confirm more to the treatment of multi-class\n",
    "            # targets.\n",
    "            label = [label]\n",
    "        label = np.array(label, dtype=np.float32)\n",
    "        time = np.array(np.arange(features.shape[0]), dtype=np.float32)[:, None]\n",
    "        features = np.array(features, dtype=np.float32)\n",
    "        return self.maybe_transform(\n",
    "            {'time': time, 'values': features, 'label': label})\n",
    "\n",
    "    \n",
    "def _to_array(data):\n",
    "        \"\"\"\n",
    "        Util function to convert iterable dataset to X,y arrays for stratified splitting\n",
    "        \"\"\"\n",
    "        X = []; y = []\n",
    "        for instance_x, instance_y in data:\n",
    "            X.append(instance_x)\n",
    "            y.append(instance_y)\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        return X, y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Provided split not available.', 'Use any of [training, validation, testing]')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-200-8b968e19335d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'PenDigits'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUEADataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-197-b15fd8ba1700>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_name, split, transform, data_path)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m#self.dataset = uea_ucr_datasets.Dataset(dataset_name, train=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUEADataReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-197-b15fd8ba1700>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset_name, split, out_path)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             raise ValueError('Provided split not available.', \n\u001b[0;32m---> 56\u001b[0;31m                              \u001b[0;34m'Use any of [training, validation, testing]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                             )\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Provided split not available.', 'Use any of [training, validation, testing]')"
     ]
    }
   ],
   "source": [
    "dataset_name = 'PenDigits'\n",
    "dataset = UEADataset(dataset_name, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-73440f1a35f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPhysionet2012Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Dataset of the PhysioNet 2012 Computing in Cardiology challenge.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     normalizer_config = os.path.join(\n\u001b[1;32m      5\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "class Physionet2012Dataset(Dataset):\n",
    "    \"\"\"Dataset of the PhysioNet 2012 Computing in Cardiology challenge.\"\"\"\n",
    "\n",
    "    normalizer_config = os.path.join(\n",
    "        os.path.dirname(__file__),\n",
    "        'resources',\n",
    "        'Physionet2012Dataset_normalization.json'\n",
    "    )\n",
    "\n",
    "    def __init__(self, split, transform=None, data_path=DATASET_BASE_PATH):\n",
    "        \"\"\"Initialize dataset.\n",
    "\n",
    "        Args:\n",
    "            split: Name of split. One of `training`, `validation`, `testing`.\n",
    "            data_path: Path to data. Default:\n",
    "                {project_root_dir}/data/physionet_2012\n",
    "\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        split_dir, split_file = self._get_split_path(split)\n",
    "\n",
    "        self.reader = PhysionetDataReader(split_dir, split_file)\n",
    "        self.feature_transform = PhysionetFeatureTransform()\n",
    "        self.normalizer = Normalizer()\n",
    "        self._set_properties()\n",
    "\n",
    "        if not os.path.exists(self.normalizer_config):\n",
    "            print(f'Normalizer config {self.normalizer_config} not found!')\n",
    "            print('Generating normalizer config...')\n",
    "            if split != 'training':\n",
    "                # Only allow to compute normalization statics on training split\n",
    "                raise ValueError(\n",
    "                    'Not allowed to compute normalization data '\n",
    "                    'on other splits than training.'\n",
    "                )\n",
    "            for i in trange(len(self)):\n",
    "                instance = self.reader.read_example(i)['X']\n",
    "                transformed = self.feature_transform(instance)\n",
    "                self.normalizer._feed_data(transformed[1])\n",
    "            self.normalizer._save_params(self.normalizer_config)\n",
    "        else:\n",
    "            self.normalizer.load_params(self.normalizer_config)\n",
    "\n",
    "        self.maybe_transform = transform if transform else lambda a: a\n",
    "\n",
    "    def _get_split_path(self, split):\n",
    "        split_paths = {\n",
    "            'training': (\n",
    "                os.path.join(self.data_path, 'train'),\n",
    "                os.path.join(self.data_path, 'train_listfile.csv')\n",
    "            ),\n",
    "            'validation': (\n",
    "                os.path.join(self.data_path, 'train'),\n",
    "                os.path.join(self.data_path, 'val_listfile.csv')\n",
    "            ),\n",
    "            'testing': (\n",
    "                os.path.join(self.data_path, 'test'),\n",
    "                os.path.join(self.data_path, 'test_listfile.csv')\n",
    "            )\n",
    "        }\n",
    "        return split_paths[split]\n",
    "\n",
    "    def _set_properties(self):\n",
    "        self.has_unaligned_measurements = True\n",
    "        self.statics = None\n",
    "        self.n_statics = 0\n",
    "        instance = self.reader.read_example(0)\n",
    "        times, features = self.feature_transform(instance['X'])\n",
    "        self._measurement_dims = features.shape[1]\n",
    "\n",
    "    @property\n",
    "    def n_classes(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def measurement_dims(self):\n",
    "        return self._measurement_dims\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.reader.get_number_of_examples()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        instance = self.reader.read_example(index)\n",
    "        t, features = self.feature_transform(instance['X'])\n",
    "        features = self.normalizer.transform(features)\n",
    "        label = instance['y']\n",
    "        if self.n_classes == 1:\n",
    "            # Add an additional dimension to the label if it is only a scalar.\n",
    "            # This makes it confirm more to the treatment of multi-class\n",
    "            # targets.\n",
    "            label = [label]\n",
    "        label = np.array(label, dtype=np.float32)\n",
    "        time = np.array(t, dtype=np.float32)[:, None]\n",
    "        features = np.array(features, dtype=np.float32)\n",
    "        return self.maybe_transform(\n",
    "            {'time': time, 'values': features, 'label': label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GP_signatures",
   "language": "python",
   "name": "gp_signatures"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
