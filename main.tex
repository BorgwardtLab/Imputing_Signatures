\documentclass{article}

\usepackage{microtype}
\usepackage{graphicx}
\usepackage{booktabs} % for professional tables
\usepackage{caption}
\usepackage{subcaption} % for subcaption boxes
\usepackage{amsmath}
\usepackage{bm}      % for bold greek letters
\usepackage{amssymb} % more math symbols
\usepackage{amsthm}
\usepackage{bbm}  % blackboard 1
\usepackage{hyperref}
\usepackage{paralist}
\usepackage{wrapfig}
\usepackage{siunitx}
\usepackage{etoolbox}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{comment}
\usepackage{multirow} 
\PassOptionsToPackage{sort, numbers, compress}{natbib}


\usepackage{neurips_2020} %[preprint]
\setcitestyle{square}

\renewcommand{\subsubsection}[1]{\textbf{#1}

} % newlines are deliberate
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\E}{\mathbb{E}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\naturals}{\mathbb{N}}
\newcommand{\sig}{\mathrm{Sig}^N}
\newcommand{\dataspace}{\mathcal{X}}
\newcommand{\lspace}{\mathcal{Y}}
\newcommand{\seriesspace}{\mathcal{S}}
\newtheorem{theorem}{Theorem}
\newcommand{\data}[1]{\mbox{\textsc{#1}}}   % can be used to typeset arbitrary data set set names

\title{Path Imputation Strategies for Signature Models of Irregular Time Series}
% Alternative Title:
%% Path Imputation Strategies with Signature Models for Irregularly Sampled Partially Observed Multivariate Time Series.

\author{
    Michael Moor
    \And
	Max Horn
	\And
	Christian Bock
	\And
	Bastian Rieck
	\And
	Karsten Borgwardt
	\AND \\[-12pt]
	Department of Biosystems Science and Engineering, ETH Zurich
	\\
	\hspace{-6.5em}\texttt{\{michael.moor, max.horn, christian.bock,}
	\\
	\hspace{6.5em}\texttt{bastian.rieck, karsten.borgwardt\}@\hspace{0.1pt}bsse.ethz.ch}
}

\begin{document}
\maketitle

\begin{abstract}
The signature transform is a `universal nonlinearity' on the space of
continuous vector-valued paths, and has received attention for use in
machine learning on time series. However, real-world temporal data is typically observed at discrete points in time, and must first be transformed into a continuous path before
signature techniques can be applied. We make this step explicit by characterising it as an imputation
problem, and empirically assess the impact of various imputation strategies when
applying signature-based neural nets to irregular time series data. For one of these
strategies, Gaussian process~(GP) adapters, we propose an
extension~(GP-PoM) that makes uncertainty information directly available
to the subsequent classifier while at the same time preventing costly
Monte-Carlo~(MC) sampling. In our experiments, we find that the choice
of imputation  drastically affects shallow signature models, whereas
deeper architectures are more robust. Next, we observe that
uncertainty-aware \emph{predictions}~(based on GP-PoM or indicator
imputations) are beneficial for predictive performance, even compared to
the uncertainty-aware \emph{training} of conventional GP adapters.
In conclusion, we have demonstrated that the path construction is indeed
crucial for signature models and that our proposed strategy leads to
competitive performance in general, while improving robustness of
signature models in particular.
\end{abstract} % TODO: touch this up a bit, but I think the essential idea is there.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Originally described by \citet{Chen54, Chen57, Chen58} and popularised
in the theory of rough paths and controlled differential
equations~\cite{lyons1998differential, FritzVictoir10, lyons2014rough},
the \emph{signature transform}, also known as the \emph{path signature}
or simply \emph{signature}, acts on a continuous vector-valued path of
bounded variation, and returns a graded sequence of statistics, which
determine a path up to a negligible equivalence class. Moreover,
\emph{every} continuous function of a path can be recovered by applying
a linear transform to this collection of statistics~\citep[Proposition
A.6]{kidger2019deep}.
%
This `universal nonlinearity' property makes the signature a promising nonparametric
feature extractor in both generative and
discriminative learning scenarios.
%
Further properties include the signature's uniqueness~\citep{hambly2010uniqueness}, as well as factorial decay of its higher order terms~\citep{lyons1998differential}. These theoretical foundations have been accompanied by outstanding empirical results when applying signatures to clinical time series classification tasks~\citep{reyna2019early, morrill2019signature}.
Due to their similarities, we may hope that tools that
apply to continuous paths can \emph{also} be applied to multivariate
time series. But since multivariate time series are not continuous
paths, one first needs to construct a continuous path before signature techniques are applicable.
%
Previous work~\citep{levin2013, kidger2019deep, fermanian2019embedding}
characterised this construction as an embedding problem, and
typically considered it a minor technical detail.
This is exacerbated by the---perfectly sensible---behaviour of software
for computing the signature~\citep{iisignature, signatory}, which
commonly considers a continuous piecewise linear path as an input,
described by its sequence of knots, i.e.\ values.
%
Since such sequences resemble a sequence of data, the signature is
sometimes interpreted as operating on sequences of data rather than on
paths~\cite{kidger2019deep, levin2013}.
%
By contrast, here we show that considering the path construction
process is crucial for achieving competitive predictive
performance: we reinterpret the task of constructing a continuous path,
turning it from an embedding problem to an imputation problem, which we
call \emph{path imputation}.
%

While previous research concerning the signature transform
focused on its excellent theoretical properties, such as sampling
independence~\citep[Proposition A.7]{kidger2019deep}, our findings show
that this does not
necessarily correspond to empirical performance.
%
We perform a thorough investigation of multiple imputation schemes in combination with various models that can potentially employ signatures.
Furthermore, motivated by the fact that missingness itself can be
informative for time series classification~\citep{rubin1976inference}, we propose a novel imputation strategy: an extension of Gaussian process adapters~\citep{li2016scalable, futoma2017mgp}, which exploits uncertainty information during each prediction step and which is beneficial for signature models, but also of independent interest.
%
We make our code anonymously available under \url{https://osf.io/bg9cw/?view_only=5193e93118d84a5f9be4f261df4c0a06}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related work}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

A key motivation for this work is the use of the signature transform in
machine learning: recent work~\citep{primer2016,
kormilitzlin2016, yang2016rotation, li2017lpsnet, yang2017leveraging,
PerezArribas2018, morrill2019sepsis} typically employed the signature transform as a nonparametric feature extractor, on top of which
a model is learnt. A growing body of work has also investigated how to
integrate the signature transform more tightly with neural networks; 
\citet{jeremythesis}, \citet{logsigrnn}, and \citet{kidger2019deep} all
study how to use the signature transform~(or variants thereof) within typical neural network models.
\citet{chevyrev2018signature, kiraly2019kernels} study how the signature
transform may be used to define a \emph{kernel}---i.e.\ a symmetric,
positive definite function that is typically used as a similarity
measure---on path space, while \citet{toth2019gp} show how this kernel
may be used to define a Gaussian process.
%
In much of this work, data has been converted into a continuous path via
linear interpolation. Some authors~\citep{primer2016,
fermanian2019embedding} have additionally considered `rectilinear'
interpolation, which is similar. \citet{levin2013} present the
`time-joined transformation', which is a hybrid of the two, such that
the resulting path exhibits a causal dependence on the data.  However,
to our knowledge, no prior work has regarded (and empirically investigated) this as an imputation
problem.

\paragraph{Imputation schemes} The general problem of imputing data is well-known and well-studied, and
we will not attempt to describe it here; see for example \citet[Chapter 25]{gelman2007dataanalysis}.
Imputation methods typically only fill in missing discrete data
points, and do not attempt to impute the underlying continuous path.
Gaussian process adapters~\citep{li2016scalable}, by contrast, are
capable of imputing a \emph{full} continuous path, from which we
may sample arbitrarily. Hence, this framework will be
considered more closely in this paper.
We note that there are also other approaches that
perform imputation end-to-end with a downstream classifier~\citep{shukla2018interpolationprediction}
and methods that skip the imputation step altogether based on recently-proposed Neural-ODE like architectures~\citep{rubanova2019latent, kidger2020neuralcde}, variants of recurrent neural networks \cite{che2018recurrent} or set functions~\cite{horn2019set}.
However, the scope of this work is to specifically assess the impact of path imputations for the signature, hence we deem the larger comparison including imputation-free scenarios interesting for future work, while it bypasses the central point of this paper.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Background: Signature transform and Gaussian process adapters} % 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Path signatures}
Let $f = (f_1, \dots, f_d) \colon [a, b] \to \reals^d$ be a continuous, piecewise
differentiable path. Then the \emph{signature transform up to depth $N$}
is
\begin{equation}\label{eq:signature}
    \sig(f)=\left(\left(\underset{\,a<t_{1}<\cdots<t_{k}<b}{\int \cdots \int} \prod_{j=1}^{k} \frac{\mathrm{d} f_{i_{j}}}{\mathrm{d} t}\left(t_{j}\right) \mathrm{d} t_{1} \cdots \mathrm{d} t_{k}\right)_{1 \leq i_{1}, \ldots, i_{k} \leq d}\right)_{1 \leq k \leq N}.
\end{equation}

This definition can be extended to paths of bounded variation by
replacing these integrals with Stieltjes integrals with respect to each
$f_{i_j}$.
%
In brief, the signature transform may be interpreted as extracting
information about \emph{order} and \emph{area} of a path.
%
One may interpret its terms as `the area/order of one channel with
respect to some collection of other channels'. To give an explicit example: first level terms simply describe the increment of the path with respect to one channel, whereas second-order terms are related to the \emph{Levy area} of the path, as shown for a one-dimensional example in Figure~\ref{fig:sig_path}.

\begin{wrapfigure}{3}{3.5cm}
    \centering
    \vspace{-1em}
	\includegraphics[width=0.25\columnwidth]{figures/sig_path1.pdf}
	\caption{Given a path (bold), its Levy area is its signed area with respect to the chord joining its endpoints.}\label{fig:sig_path}
	\vspace{-2em}
\end{wrapfigure}

For an exposition on the properties of the signature transform and its use in machine learning, please refer to \citet{primer2016} or \citet[Appendix A]{kidger2019deep}. For building more intuition, in Section~\ref{sec: comparison fourier} of the appendix, we compare the signature to more well-known transforms.

\paragraph{Computing the signature transform} \label{sec: computing sig}
Continuous piecewise linear paths are the paths of choice, computationally speaking, due to the fact that this is the only case for which efficient algorithms for computing the signature transform are known \cite{signatory}.

This is not a serious hurdle when one wishes to compute the signature of a path $f$ that is not piecewise linear---as the signature of piecewise linear approximations to $f$ will tend towards the signature of $f$ as the quality of the approximation increases---but it does enforce this requirement on our imputation schemes.
Thus, all of the imputation schemes we examine will first seek to select a collection of points in data space (not necessarily only where we had data before), and for computing the signature we join them up into a piecewise linear path.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Notation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
We define the space of time series over a set $A$ by
%
\begin{equation}
    \seriesspace(A) = \{((t_1, x_1), \ldots, (t_n, x_n)) \,\vert\, t_i \in \reals, x_i \in A, n \in \naturals, \text{ such that } t_1 \leq \cdots \leq t_n\}.\label{eq:seriesspace}
\end{equation}

Furthermore, let $\lspace$ be a set and let $\dataspace_j = \reals$ for $j \in \{1,
\ldots, d\}$ and $d \in \naturals$. Then we assume that we observe
a dataset of labelled time series $(\mathbf{x}_k, y_k)$ for $k \in \{1,
\ldots, N\}$, where $\mathbf{x}_k \in \seriesspace(\dataspace^*)$ and
$y_k \in \lspace$, with $\dataspace^* = \prod_{j = 1}^d(\dataspace_j
\cup \{*\})$ and $*$ representing no observation.
%
We similarly define $\dataspace = \prod_{j = 1}^d\dataspace_j$. Thus,
$\dataspace$ is the data space, while $\dataspace^*$ is the data space
allowing missing data, and $\lspace$ is the set of labels.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Gaussian process adapter} \label{sec: GPadapter}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
Some of the imputation schemes we consider are based on
the uncertainty aware-framework of multi-task Gaussian process
adapters~\citep{li2016scalable, futoma2017mgp}.
%
Let $\mathcal{W}, \mathcal{H}$ be some sets. Let $\ell \colon \lspace
\times \lspace \to [0, \infty)$ be a loss function. Let $F \colon
\dataspace^{[a, b]} \times \mathcal{W} \to \lspace$, be some (typically
neural network) model, with $\mathcal{W}$ interpreted as a space of
parameters. Let \begin{align*} \mu \colon [a, b] \times
\seriesspace(\dataspace^*) \times \mathcal{H} &\to \dataspace\\ \Sigma
\colon [a, b] \times [a, b] \times \seriesspace(\dataspace^*) \times
\mathcal{H} &\to \dataspace    \end{align*} be mean and covariance
functions, with $\mathcal{H}$ interpreted as a space of
hyperparameters. The dependence on $\seriesspace(\dataspace^*)$ is used to
represent conditioning on observed values.

Then the goal is to solve
\begin{equation}\label{eq:gp-mc}
\argmin_{\mathbf{w} \in \mathcal{W},\bm{\eta} \in \mathcal{H}} \sum_{k=1}^N \overbrace{\rule{0pt}{0.5cm} \mathbb{E}_{\mathbf{z}_k \sim \mathcal{N}\left( \mu(\cdot, \mathbf{x}_k, \eta), \Sigma(\cdot, \cdot, \mathbf{x}_k, \eta)\right) } \big[ \ell(F(\mathbf{z}_k, \mathbf{w}),
y_k) \big] }^{\text{$E_k$}}.
\end{equation}
%
As this expectation is typically not tractable, it is estimated by MC
sampling with $S$ samples, i.e.\
%
\begin{equation}
E_k \approx \frac{1}{S} \sum_{s=1}^{S} \ell(F(\mathbf{z}_{s, k}, \mathbf{w}), y_k),
\end{equation}
%
where
\begin{equation}
    \mathbf{z}_{s, k} \sim \mathcal{N}\left( \mu(\,\cdot\,, \mathbf{x}_k, \eta), \Sigma(\,\cdot\,, \,\cdot\,, \mathbf{x}_k, \eta)\right).
\end{equation}
%
Alternatively, one may forgo allowing the uncertainty to propagate through $F$ by instead passing the posterior mean directly to $F$; this corresponds to solving
\begin{equation}\label{eq:gp-mean}
  \argmin_{\mathbf{w} \in \mathcal{W},\bm{\eta} \in \mathcal{H}} \sum_{k=1}^N \ell(F(\mu(\,\cdot\,,\mathbf{x}_k, \eta), \mathbf{w}), y_k).
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Path imputations for signature models}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Signatures act on continuous paths. However, in real-world
applications, temporal data typically appears as a discretised
collection of measurements, potentially irregularly-spaced and
asynchronously observed. To apply the signature to this data,
it first has to be converted into a continuous path. 
%
We believe this step to have a significant impact on the
resulting signature, and thus also on models employing the signature.
To assess this hypothesis, we explicitly treat this transformation
as a \emph{path imputation}, i.e.\ a mapping of the form $ \phi \colon \seriesspace(\dataspace^*) \to (\reals \times \dataspace)^{[a, b]}.$
%
\paragraph{Task} We aim to learn a function $ g \colon \seriesspace(\dataspace^*) \to
\lspace$, which decomposes to $g = F \circ \phi$, where $F$ refers to
a classifier, mapping from $(\reals \times \dataspace)^{[a, b]} \times \mathcal{W} $ to
$\lspace$. Given a loss function $\ell$ and a set of $p$ path
imputation strategies, ${\Phi} =(\phi_i)_{i=1}^p$, we seek to minimise the
objective:
%
\begin{equation}
    \argmin_{\phi_i \in \Phi, \mathbf{w} \in \mathcal{W}} \quad \mathbb{E}_{(\mathbf{x},y) \sim P(\seriesspace(\dataspace^*),\lspace) } \left[ \ell( g(\mathbf{x}; \phi_i, \mathbf{w}), y) \right]
    \label{eq:objective}
\end{equation} 
%
Even though Equation~\eqref{eq:objective} could be formulated more
\emph{implicitly}~(i.e.\ without any explicit imputation step),
this formulation enables us to make explicit how the signature transform
`interprets' the raw data for downstream classification tasks. We further motivate this need for assessing the path construction in Section~\ref{sec:Fragile dependence} by showing that a single imputed value can affect the Levy area which is computed with the signature. 

\paragraph{Path imputation strategies}
For our analysis, we consider the following set of strategies for path imputation, namely
\begin{inparaenum}[(1)]
    \item linear interpolation,
    \item forward filling,
    \item indicator imputation,
    \item zero imputation,
    \item causal imputation\footnote{This strategy is similar to the
      time-joined transformation \citep{levin2013}. For more details,
    please refer to Section \ref{sec:Causal signature imputation} in the
  appendix.}, and
    \item Gaussian process adapters~(GP).
\end{inparaenum}
%
Strategies 1--5 can be seen as a fixed preprocessing step, whereas GP
adapters~(strategy~6) are optimised end-to-end with the downstream task. For more
details regarding these strategies, please refer to Section~\ref{supp: Imputation} in the appendix. As indicated in Section~\ref{sec: computing sig}, for computing the signature efficiently~(i.e.\ computed in terms of standard tensor operations \citep[Proposition A.3]{kidger2019deep}), the imputed time series are transformed into piecewise linear paths beforehand.

\begin{figure}[t]
    \centering
    \hspace*{1cm}
    \includegraphics[width=0.7\columnwidth]{figures/overview.pdf}
    \caption{%
      Overview of our proposed extension of GP adapters, GP-PoM,
      leveraging both posterior moments~(mean and variance). In
      comparison, the conventional GP adapter feeds MC samples~(faded
      colours in the background) drawn from the GP posterior into the
      classifier.
    }
    \label{fig:overview}
\end{figure}

\paragraph{GP adapter with posterior moments}
%
For conventional GP adapters, one major drawback with the formulations
of \citet{li2016scalable} and \citet{futoma2017mgp}, as described in
Equation~\eqref{eq:gp-mc}, is that approximating the expectation outside
of the loss function with MC sampling is expensive.
%
During prediction, \citet{li2016scalable} proposed to overcome this
issue by sacrificing the uncertainty in the loss function and to simply
pass the posterior mean, as in Equation~\eqref{eq:gp-mean}\footnote{%
  Equations \eqref{eq:gp-mc} and \eqref{eq:gp-mean} are of course not in
  general equal, so following \citet{futoma2017mgp}, our standard GP
  adapter uses MC sampling both in training and testing.
}.
%
To address both points, we propose to instead also pass the posterior
covariance of the Gaussian process to the classifier $F$. This saves the
cost of MC sampling whilst explicitly providing~$F$ with uncertainty
information during the prediction\footnote{%
Even if MC sampling
is used during prediction, $F$ has no per-sample access to
uncertainty about the imputation.
}.
%
However, the full covariance matrix may become very large, and it is
not obvious that all interactions are relevant to the subsequent
classifier.
This is  why we simplify matters by taking the posterior
\emph{variance} at every point, and concatenate it with the posterior
mean at every point, to produce a path whose evolution also captures the
uncertainty at every point:
%
\begin{align}
    \tau &\colon [a, b] \times \seriesspace(\dataspace^*) \times \mathcal{H} \to \dataspace \times \dataspace\\
    \tau &\colon t, \mathbf{x}, \eta \mapsto (\mu(t, \mathbf{x}, \eta), \Sigma(t, t, \mathbf{x}, \eta)).
\end{align}
%
This corresponds to solving
%
\begin{equation}\label{eq:gp-moments}
\argmin_{\mathbf{w} \in \mathcal{W},\bm{\eta} \in \mathcal{H}} \sum_{k=1}^N \ell(F(\tau(\,\cdot\,,\mathbf{x}_k, \eta), \mathbf{w}), y_k),
\end{equation}
%
where instead now $F \colon (\dataspace \times \dataspace)^{[a, b]}
\times \mathcal{W} \to \lspace$.
%
As this approach leverages information from both posterior moments (mean and variance), we refer to it as posterior moments GP adapter, or short \textsc{GP-PoM}.
%
Figure~\ref{fig:overview} gives an overview of \textsc{GP-PoM}. 
%
In our context of interest, when $F$ is a signature model, it is now
straightforward to compute the signature of the Gaussian process, simply
by querying many points to construct a piecewise linear approximation to
the process.  The choice of kernel has non-trivial mathematical
implications for this procedure: for example if a Mat{\'e}rn 1/2 kernel
is chosen, then the resulting path is not of bounded variation and the
definition of the signature transform given in Equation~\eqref{eq:signature}
does not hold, and rough path theory~\citep{lyons1998differential} must instead be invoked to define the
signature transform. However, in this work we use RBF kernels, and therefore, this caveat does not apply to our case.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We first introduce our experimental setup~(datasets and model
architectures) before presenting and discussing quantitative results.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Datasets and preprocessing}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We classify time series from four real-world datasets, i.e.\
%
\begin{inparaenum}[(i)]
  \item \texttt{Physionet2012}~\citep{goldberger2000physiobank},
  \item \texttt{PenDigits}~\citep{Dua2019},
  \item \texttt{LSST}~\citep{allam2018photometric}, and
  \item \texttt{CharacterTrajectories}~\citep{Dua2019}.
\end{inparaenum}
%
For dataset statistics and necessary filtering steps, please refer to Section \ref{supp: Dataset stats} in the appendix.
Moreover, to efficiently compute the signature, we
sample the imputed path in a \emph{fixed} time resolution\footnote{For Physionet2012 hourly, for the other datasets once per originally observed time step}, resulting in
a piecewise linear path.
%
For time series that are not irregularly spaced~(this applies to all datasets but \texttt{Physionet2012}), we employ two types of random subsampling as an additional
preprocessing step,
namely
%
\begin{inparaenum}[(1)]
    \item `Random': Missing at random; on the instance level, we discard 50\% of all observations.
    \item `Label-based': Missing not at random; for each
      class, we uniformly sample missingness frequencies between $40\%$
      and $60\%$.
\end{inparaenum}
%
Since \texttt{PenDigits} consists of particularly short time series~(8
steps, 2 dimensions), we use more moderate frequencies of $30\%$
and $20$--$40\%$, respectively, for discarding observations.
Finally, we standardise all time series channels using the empirical mean and standard deviation as determined on the entire training split.

\paragraph{Models}
%
We study the following models:
\begin{inparaenum}[(1)]
  \item \textsc{Sig}, a simple signature model that involves a linear
    augmentation, the signature transform~(signature block) and a final
    module of dense layers,
  %
  \item \textsc{RNN}, an RNN model using GRU cells~\citep{cho2014learning},
  %
  \item \textsc{RNNSig}, which extends the signature transform to a window-based
    stream of signatures, and where the final neural module is a GRU
    sliding over the stream of signatures, and
  %
  \item \textsc{DeepSig}, a deep signature model sequentially employing two signature blocks featuring augmentation and signature transforms,
  following \citet{kidger2019deep}.
\end{inparaenum}
%
Please refer to Supplementary Section~\ref{supp: Model Architectures} for more details about the
architectures and implementations. We use the `Signatory' package to
calculate the signature transform~\citep{signatory}, and implemented all GP
adapters using the `GPyTorch' framework~\citep{gardner2018gpytorch}.

\paragraph{Training and evaluation}
%
We use the predefined training and testing splits for each dataset,
separating $20\%$ of the training split as a validation set for
hyperparameter tuning.
%
For each setting, we run a randomised hyperparameter search of $20$
calls and train each of these fits until convergence~(at most 100
epochs; we stop early if the performance on the validation
split does not improve for 20 epochs). As for performance metrics, for
binary classification tasks, we optimise area under the precision-recall curve (as approximated via average precision) and also report AUROC. For multi-class
classification, we optimise balanced accuracy~(BAC) and additionally
report accuracy and weighted AUROC~(w-AUROC)\footnote{%
  AUROC is computed for each label and averaged with weights according to
  the support of each class%
}.
%
Having selected the best hyperparameter configuration for each setting,
we repeat $5$ fits; for each fit, we select the best model state in
terms of the best validation performance, and finally report mean and
standard deviation~(error bars) of the performance metrics on the
testing split.

\begin{table}[tbp] %
    \caption{\texttt{CharacterTrajectories} dataset under label-based subsampling. The top three methods are highlighted: bold \& underlined, bold, underlined. All measures are reported as percentage points. Balanced accuracy (BAC) is the metric we optimised for. We further report accuracy and weighted AUROC (w-AUROC).}
    \centering
    \input{tables/repetitions_CharacterTrajectories_Label-based}
    \label{tab:character}
\end{table}

\begin{table}[tbp]
    \caption{\texttt{PenDigits} dataset under label-based subsampling. The top three methods are highlighted: bold \& underlined, bold, underlined. All measures are reported as percentage points. Balanced accuracy (BAC) is the metric we optimised for. We further report accuracy and weighted AUROC (w-AUROC)}
    \centering
    \input{tables/repetitions_PenDigits_Label-based}
    \label{tab:pendigits}
\end{table}

\paragraph{Results}
%
\begin{figure*}[tbp]
  \subcaptionbox{CharacterTrajectories-R}{%
    \includegraphics[width=0.48\linewidth]{plots/barplot_main_CharacterTrajectories-MissingAtRandomSubsampler.pdf}\quad%
   }%
  \subcaptionbox{CharacterTrajectories-L}{%
    \includegraphics[width=0.48\linewidth]{plots/barplot_main_CharacterTrajectories-LabelBasedSubsampler.pdf}\quad%
  }
  \caption{Experimental results visualised for \texttt{CharacterTrajectories} dataset.
  The bars display performance in terms of balanced accuracy (BAC),
whereas the panels indicate the subsampling strategy.
  Left: Random subsampling (R), right: label-based subsampling (L).}
  \label{fig:results_main}
\end{figure*}

In Tables \ref{tab:character} and \ref{tab:pendigits}, the results for
\texttt{CharacterTrajectories} and \texttt{PenDigits} under label-based
subsampling are shown, respectively. For the remaining datasets and
subsampling strategies, please refer to Tables~\ref{supp: tab
character}--\ref{supp: tab physionet} in the appendix. 
We observe that both \textsc{DeepSig} as well as the signature-free \textsc{RNN} perform well
over many scenarios. In particular, they are impervious to the choice of
several imputation schemes in the sense that it does not have a large
impact on their predictive performance.
%
However, we also see that certain signature models, in particular
\textsc{Sig}, are heavily impacted by the choice of imputation strategy.
Figure~\ref{fig:results_main} exemplifies this finding in a barplot
visualization; for the remaining visualizations, including the number of
parameters of the optimised models, please refer to Supplementary
Figures~\ref{supp: barplots1} and~\ref{supp: barplots2}. In the case of
\texttt{CharacterTrajectories}, \textsc{Sig} was only able to achieve
acceptable performance through our novel \textsc{GP-PoM} strategy. In
\texttt{PenDigits}, we encountered issues of numerical stability for the
original GP adapter\footnote{They were addressed by jittering the
diagonal in the Cholesky decomposition.}; not so for \textsc{GP-PoM}.
Furthermore, we found that \textsc{GP-PoM} tends to converge faster to
a better performance than the original GP adapter, as exemplified in
Supplementary Figure~\ref{supp: gp-training}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Our findings suggest that the choice of path imputation strategy can
\emph{drastically} affect the performance of signature-based models. We
observe this most prominently in `shallow' signature models, whereas deep signature models~(\textsc{DeepSig})
are more robust in tackling irregular time series over different
imputations---comparable to non-signature RNNs, yet on average being
more parameter-efficient.

Overall, we find that uncertainty-aware approaches~(indicator
imputation and \textsc{GP-PoM}) are beneficial when imputing
irregularly-spaced time series for classification.
%
Crucially, uncertainty information has to be accessible during the
\emph{prediction step}. We find that this is indeed not the case for
the standard GP adapter~(despite the naming of `uncertainty-aware
framework'), since for each MC sample, the downstream classifier has no access
to missingness or uncertainty about the underlying imputation.
\textsc{GP-PoM}, our proposed end-to-end imputation strategy, shows
competitive classification performance, while considerably improving
upon the existing GP adapter. As for
limitations, \textsc{GP-PoM} sacrifices the GP adapter's ability to be
explicitly uncertain \emph{about} its own prediction~(due to the
variance of the MC sampled predictions), while the subsequent classifier
has to be able to handle the doubled feature dimensionality.

\paragraph{Recommendations for the practitioner}
%
When dealing with a challenging time series classification task, we recommend to consider signatures as a powerful tool to encode paths with little loss of information. However, we observe that this comes at a certain cost: since the signature describes continuous paths~(and not discrete time series), constructing this path from raw data is a delicate task that can heavily impact the signature and the performance of downstream models. To this end, we recommend using \textsc{GP-PoM}, which explicitly captures uncertainty in the imputed path. Given our findings, indicator imputation is a simple but promising go-to strategy, however we caution its use together with shallow signature models since we observed detrimental effects in terms of predictive performance.
%
Furthermore, when applying signatures in online applications or settings, where during training no data should leak from the future~(e.g.\ in online settings, this could impair performance upon deployment), we recommend to use causal~(or time-joined) path imputations: their design specifically prevents leakage from the future, even if the signature interprets the imputations as knots of a piece-wise linear path. 

\section{Conclusion}

The signature transform has recently gained attention for being
a promising feature extractor that can be easily integrated to neural
networks. As we empirically demonstrated in this paper, the application
of signature transforms to real-world temporal data is fraught with
pitfalls---specifically, we found the choice of an imputation scheme to
be crucial for obtaining high predictive performance. Moreover, by
integrating uncertainty to the prediction step, our proposed
\textsc{GP-PoM} has demonstrated overall competitive performance and in
particular improved robustness in signature models when classifying irregularly-spaced and asynchronous time series. 

\section*{Broader Impact}
Whilst the task of converting observed data into a path in data space is particularly important for signatures, it also arises in the context of, for example, convolutional and recurrent neural networks.

Convolutions are often thought of in terms of discrete sums, but they are perhaps more naturally described as the integral cross-correlation between the underlying data path $f$ and the learnt filter $g_\theta$. Given sample points $t_1, \ldots, t_n \in [0, T]$, this integral is then approximated via numerical quadrature:
\begin{equation*}
    \frac{1}{T}\int_0^T f(t) g_\theta(t) \mathrm{d}t \approx \frac{1}{n}\sum_{i = 1}^n f(t_i) g_\theta(t_i),
\end{equation*}
although the $1/n$ scaling is really only justified in the case that the $t_i$ are equally spaced.\footnote{The $g_\theta$ is typically a step function in `normal' convolutional layers. Some works exists on replacing it with e.g. B-splines \cite{fey2018splinecnn} to better handle irregular data. The oddity of scaling by $1/n$ with irregular data has not been explicitly addressed in the literature, at least to our knowledge; indeed quite conversely we have seen it used without remark.} Thus we see that with convolutions, we are implicitly interpreting the observed data as a path in data space.

Similarly, the connections between dynamical systems and recurrent neural
networks are well known~\citep{FUNAHASHI1993801, continuousrnn}, and these tend to use a similar setup.
%
For non-signature methods as for signature methods, this implicit usage of data as a path in data space often seems to be swept under the rug, and we have demonstrated that this is deserving further attention.

With respect to ethical considerations, we acknowledge that time series models in general can be used for better or for worse. On top of that, implicit biases underlying models as well as datasets can have unintended harmful consequences. By shedding light on the impact of implicit usage of paths in data space we hope to forward understanding and accountability of black-box models. Even if our work focuses on signature models, we believe that the principle of making underlying assumptions explicit is relevant far beyond this class of models. 

\begin{ack}
M.M, M.H, and C.B were supported by the SNSF Starting Grant `Significant
Pattern Mining'. The authors are grateful to Patrick Kidger for valuable discussions, guidance, and code contributions.
\end{ack}

\bibliography{ref}
\bibliographystyle{apalike}
\newpage
\appendix

\section{Appendix}\label{sec:Appendix}

\subsection{Further Experiments}\label{supp:Experiments}

\begin{table}[htbp]
    \begin{center}
	\caption{\texttt{CharacterTrajectories}, random subsampling}
	\input{tables/repetitions_CharacterTrajectories_Random}
	\label{supp: tab character}
	\end{center}
\end{table}

\begin{table}[htbp]
    \begin{center}
	\caption{\texttt{PenDigits}, random subsampling}
	\input{tables/repetitions_PenDigits_Random}
	\label{supp: tab pendigits}
	\end{center}
\end{table}

\begin{table}[htbp]
    \begin{center}
	\caption{\texttt{LSST}, label-based subsampling}
	\input{tables/repetitions_LSST_Label-based}
	\label{supp: tab lsst label-based}
	\end{center}
\end{table}

\begin{table}[htbp]
    \begin{center}
	\caption{\texttt{LSST}, random subsampling}
	\input{tables/repetitions_LSST_Random}
	\label{supp: tab lsst random}
	\end{center}
\end{table}

\begin{table}[htbp]
    \begin{center}
	\caption{\texttt{Physionet 2012} }
	%\small{
	\input{tables/repetitions_Physionet2012}
	\label{supp: tab physionet}
	%}
	\end{center}
\end{table}


\begin{figure}[tbp]
\subcaptionbox{CharacterTrajectories-R}{%
    \includegraphics[width=0.48\linewidth]{plots/barplot_main_CharacterTrajectories-MissingAtRandomSubsampler.pdf}\quad%
     \includegraphics[width=0.48\linewidth]{plots/barplot_main_params_CharacterTrajectories-MissingAtRandomSubsampler.pdf}%
   }\\%
  \subcaptionbox{CharacterTrajectories-L}{%
    \includegraphics[width=0.48\linewidth]{plots/barplot_main_CharacterTrajectories-LabelBasedSubsampler.pdf}\quad%
     \includegraphics[width=0.48\linewidth]{plots/barplot_main_params_CharacterTrajectories-LabelBasedSubsampler.pdf}%
  }
  \subcaptionbox{LSST-R}{%
    \includegraphics[width=0.48\linewidth]{plots/barplot_main_LSST-MissingAtRandomSubsampler.pdf}\quad%
    \includegraphics[width=0.48\linewidth]{plots/barplot_main_params_LSST-MissingAtRandomSubsampler.pdf}%
  }\\%
  \subcaptionbox{LSST-L}{%
    \includegraphics[width=0.48\linewidth]{plots/barplot_main_LSST-LabelBasedSubsampler.pdf}\quad%
    \includegraphics[width=0.48\linewidth]{plots/barplot_main_params_LSST-LabelBasedSubsampler.pdf}
  }%
  \caption{%
    Visualisations for \texttt{CharacterTrajectories} and \texttt{LSST} . The rows indicate datasets and different subsampling schemes (R for Random, L for Label-based). The left column displays the performance metric which was optimzied for: balanced accuracy (BAC), or average precision. The right column indicates the number of trainable parameters which the best model required (as selected in the hyperparameter search).
  }
  \label{supp: barplots1}
\end{figure}
\begin{figure}
   \subcaptionbox{PenDigits-R}{%
     \includegraphics[width=0.48\linewidth]{plots/barplot_main_PenDigits-MissingAtRandomSubsampler.pdf}\quad%
     \includegraphics[width=0.48\linewidth]{plots/barplot_main_params_PenDigits-MissingAtRandomSubsampler.pdf}
   }\\%
  \subcaptionbox{PenDigits-L}{%
    \includegraphics[width=0.48\linewidth]{plots/barplot_main_PenDigits-LabelBasedSubsampler.pdf}\quad%
    \includegraphics[width=0.48\linewidth]{plots/barplot_main_params_PenDigits-LabelBasedSubsampler.pdf}
  }\\ %
  \subcaptionbox{Physionet2012}{%
    \includegraphics[width=0.48\linewidth]{plots/barplot_main_Physionet2012-.pdf}\quad%
    \includegraphics[width=0.48\linewidth]{plots/barplot_main_params_Physionet2012-.pdf}
  }
  \caption{%
    Visualisations for \texttt{PenDigits} and \texttt{Physionet} . The rows indicate datasets and different subsampling schemes (R for Random, L for Label-based). The left column displays the performance metric which was optimzied for: balanced accuracy (BAC), or average precision. The right column indicates the number of trainable parameters which the best model required (as selected in the hyperparameter search).
  }
  \label{supp: barplots2}
\end{figure}

\begin{figure}[tbp] 
    \begin{center}
    \includegraphics[width=0.95\linewidth]{plots/gp_training_plot.pdf}\quad%
  \end{center}
  \caption{GP-PoM training illustrated for CharacterTrajectories as compared to conventional GP adapter.}
  \label{supp: gp-training}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Imputation strategies} \label{supp: Imputation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We consider the following set of strategies for path imputation, i.e.\
\begin{enumerate}
    \item linear interpolation: At a given imputation point, the previous and next observed data point are linearly interpolated. Missing values at the start or end of the time series are imputed with $0$ which for standardised data also corresponds to the mean.
    \item forward filling: At a given imputation point, the last observed value is carried forward. Missing values at the start of the time series are imputed with $0$.
    \item indicator imputation: At a given imputation point, for each feature dimension, if no observation is available a binary missingness indicator variable is set to $1$, $0$ otherwise. The missing value is filled with $0$.
    \item zero imputation: At a given imputation point, missing values are filled with $0$.
    \item causal imputation: This approach is related to forward filling
      and motivated by signature theory. As opposed to forward filling,
      the time and the actual value are updated sequentially. For more
      details, we introduce causal imputation in Section~\ref{sec:Causal signature imputation}.
    \item Gaussian process adapter: We introduce GP adapters in
      Section~\ref{sec: GPadapter}, where $\mathbf{z}$
      refers to the imputed time series~(modelled as Gaussian
      distribution).
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Dataset statistics and filtering} \label{supp: Dataset stats}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\paragraph{Physionet2012}
As our focus is time series classification, for \texttt{Physionet2012}~\citep{goldberger2000physiobank}, we included the 36 time series variables, and excluded the static covariates (notably, we counted the variable `weight' as a static covariate). Subsequently, we excluded the following $12$ icu stays (here represented by there ids) for having no time series data (but only static covariates): $140501, 150649, 140936, 143656, 141264, 145611, 142998, 147514,$  $142731, 150309, 155655, 156254 $, and a single noisy encounter, $135365$, which contained much more observations than all other patients. After these filtering steps, we count $11987$ instances and a binary class label, whether a patient survives the hospital stay or not.

\paragraph{PenDigits}
For \texttt{PenDigits}~\citep{Dua2019}, we count $10992$ samples, featuring $2$ channels and $8$ time steps, and $10$ classes.

\paragraph{LSST}
LSST~\citep{allam2018photometric} contains $4925$ instances featuring $6$ channel dimensions and $36$ time steps. This dataset contains $14$ classes.

 \paragraph{CharacterTrajectories}
 This dataset contains $2858$ instances, featuring $3$ channel dimensions, $182$ time steps and $20$ classes~\citep{Dua2019}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Model implementations, architectures and hyperparameters} \label{supp: Model Architectures}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

All models are implemented in Pytorch~\citep{pytorch2019}, whereas the
GP adapter and \textsc{GP-PoM} are implemented using the GPyTorch
framework~\citep{gardner2018gpytorch}. Next, we specify the details of
the model architectures.

\paragraph{\textsc{Sig}}
We use a simple signature model that involves one        signature block comprising of a linear
    augmentation followed by the signature transform. Subsequently, a final module of dense layers $(30,30)$ is used. This is architecture refers to the Neural-signature-augment model \cite{kidger2019deep}.
  %
  \paragraph{\textsc{RNNSig}} This model extends the signature transform to a window-based
    stream of signatures, where the final neural module is a GRU
    sliding over the stream of signatures. We allowed window sizes between $3$ and $10$ steps. For the GRU cell, we allowed any of the following number of hidden units: $[16,32,64,128]$.
    
  \paragraph{\textsc{RNN}} Here, we use a standard RNN model using GRU cells. The size of hidden units was chosen as one of the following: $[16,32,64,128, 256, 512]$.
  %
  \paragraph{\textsc{DeepSig}} For the deep signature model we  employ two signature blocks (each comprising a linear augmentation and the signature calculation) following \citet{kidger2019deep}.

\subsubsection{Hyperparameters}

For all signature-based models, we allowed a signature truncation
depth of $2$--$4$, as we observed that larger values quickly led to
a parameter explosion. All models were optimised using Adam
\citep{kingma2014adam}. Both the learning rates and weight decay
were drawn log-uniformly between $10^{-4}$ and $10^{-2}$. We allowed
for the following batch-sizes: $(32, 64, 128, 256)$. For GP-based
models, to save memory, we used virtual batching based on
a batch-size of~$32$. Furthermore, for standard GP adapters we used $10$ MC samples, conforming with recent literature \citep{futoma2017mgp, moor2019early}. All approaches were constrained to have no more than $1.5$ million trainable parameters.
    

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Fragile dependence on sampling in unrelated channels: example}
\label{sec:Fragile dependence} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
    \centering
    \includegraphics[width=0.35 \columnwidth]{figures/sig_path3.pdf}
    \caption{
      L{\'e}vy area of the forward-fill imputed path. By changing
      $t_{3/2}$~(a \emph{single} unrelated observation!), we can make
      this disparity greater or smaller.
    }
    \label{fig:bentline}
\end{figure}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Suppose that we have observed the~(very short) time series
%
\begin{equation}\label{eq:flaw1}
    \mathbf{x} = ((t_1, x_1^1, x_1^2), (t_2, x_2^1, *)) \in \seriesspace(\reals^2).
\end{equation}
%
Perhaps we now apply, say, forward fill data-imputation, to produce
%
\begin{equation*}
    ((t_1, x_1^1, x_1^2), (t_2, x_2^1, x_1^2)).
\end{equation*}
%
Finally we linearly path-impute to create the linear path
%
\begin{align*}
    f &\colon [t_1, t_2] \to \reals \times \reals^2\\
    f &\colon t \mapsto \left(t, x_1^1\frac{t_2 - t}{t_2 - t_1} + x_2^1\frac{t - t_1}{t_2 - t_1}, x_1^2\right),
\end{align*}
%
to which we may then apply the signature transform. In particular we
will have computed the L{\'e}vy area with respect to $t$ and $x^1$. As this
is just a straight line, the L{\'e}vy area is zero.

Now suppose we include an additional observation at some time $t_{3/2} \in (t_1, t_2)$, so that our data is instead
%
\begin{equation}\label{eq:flaw2}
    \mathbf{x} = ((t_1, x_1^1, x_1^2), (t_{3/2}, *, x_{3/2}^2), (t_2, x_2^1, *)).
\end{equation}
%
Then the same procedure as before will produce the data
%
\begin{equation*}
    \mathbf{x} = ((t_1, x_1^1, x_1^2), (t_{3/2}, x_1^1, x_{3/2}^2), (t_2, x_2^1, x_{3/2}^2)),
\end{equation*}
%
with corresponding function $f$. The $(t, x^1)$ components of $f$ and
its $(t, x^1)$-L{\'e}vy area are shown in Figure~\ref{fig:bentline}. As
a result of an unrelated observation in the $x^2$ channel, the $(t,
x^1)$-L{\'e}vy area has been changed.
%
The closer $t_{3/2}$ is to $t_2$, the greater the disparity.
%
This simple example underscores the danger of `just forward-fill
data-imputing'. Doing so has introduced an undesired dependency on the
simple \emph{presence} of an observation in other channels, with the
change in our imputed path being determined by the \emph{time} at which
this other observation occurred.

Indeed, \emph{any} imputation scheme that predicts something other than
the unique value lying on the dashed line in Figure~\ref{fig:bentline},
will fail. This means that this example holds for essentially every
data-imputation scheme---the only scheme that survives this flaw is the
linear data-imputation scheme. This is the unique imputation scheme
that coincides with the linear path-imputation that \emph{must} be our
concluding step.
%
However, when there is missing data at the start or the end of
a partially observed times series, then there is no `next observation'
which linear imputation may use. So in general, we cannot uniformly
apply the linear data-imputation scheme, and must choose another scheme or find ad-hoc solutions for missing data at the start or the end of the time series. Furthermore, it is plausible to assume that linear interpolation suffers from low expressivity as an imputation scheme which might empirically mask this benefit.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Causal signature imputation}\label{sec:Causal signature imputation}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In Section \ref{sec:Fragile dependence} we have spoken about the limitations of traditional data-imputation schemes, and at first glance one may be forgiven for
thinking that these are issues are unavoidable. 
%
However, it turns out that we need not be limited just to these
traditional imputation schemes. The trick is to consider time not as
a \emph{parameterisation}, but as a \emph{channel}\footnote{To be clear, using time as a channel is already
a well-known trick in the signature literature that we do not take
credit for inventing! See for example \citet[Definition
A.3]{kidger2019deep}. It is however pleasing that something commonly
used in the theory of signatures is also what allows us to overcome what
we identify as some of their limitations.}.
%
This leads to a `meta imputation strategy', which we refer to as
\emph{causal signature imputation}. It will turn any traditional causal
data-imputation strategy (for example, feed-forward) into a causal
path-imputation strategy for signatures; at the same time it will
overcome the issue of a fragile dependence.

Suppose we have $\mathbf{x} \in \seriesspace(\dataspace^*)$, and some
favourite choice of causal data-imputation strategy $c \colon
\seriesspace(\dataspace^*) \to \seriesspace(\dataspace)$.
%
Next, given
%
\begin{align}
    \mathbf{x} = ((t_1, x_1), \ldots, (t_n, x_n)) \in \seriesspace(\dataspace),
\end{align}
%
we define the operation $\Omega \colon \seriesspace(\dataspace) \to \seriesspace(\dataspace)$ by
%
\begin{align}
    \Omega(\mathbf{x}) = (&(t_1, x_1), (t_2, x_1), (t_2, x_2),(t_3, x_2),\nonumber\\
    &\ldots,\nonumber\\
    &(t_i, x_i), (t_{i + 1}, x_i), (t_{i + 1}, x_{i + 1}), (t_{i + 2}, x_{i + 1}),\nonumber\\
    &\ldots,\nonumber\\
    &(t_{n - 1}, x_{n - 1}), (t_n, x_{n - 1}),(t_n, x_n)).\label{eq:causalsig}
\end{align}
%
That is, \emph{first} time is updated, and \emph{then} the corresponding observation
in data space is updated. This means that the change in data space
occurs instantaneously.

For each $n \in \naturals$~(and given $a < b$), fix any $s_i^{(n)}$ for
$i \in \{1, \ldots, n \}$. 
(We will see that the exact choice is unimportant in a moment.)
%
Given
%
\begin{align*}
    \mathbf{x} = ((t_1, x_1), \ldots, (t_n, x_n)) \in \seriesspace(\dataspace),
\end{align*}
%
let $\psi \colon \seriesspace(\dataspace) \to (\reals \times
\dataspace)^{[a, b]}$ be the unique continuous piecewise linear path
such that $\psi(s_i^{(n)}) = (t_i, x_i)$. Note that this is just
a slight generalisation of the linear path-imputation that has already
been performed so far; we are simply no longer asking for additional
assumptions of the form $s_i^{(n)} = t_i$.\footnote{As in the
$\mathbf{\varphi}_\theta$ of \cite{toth2019gp}, for example.}

Finally, we put this all together, and define the causal signature imputation strategy $\phi_c$ associated with $c$ to be
%
\begin{equation*}
\phi_c = \psi \circ \Omega \circ c,
\end{equation*}
which will be a map $\seriesspace(\dataspace^*) \to (\reals \times \dataspace)^{[a, b]}$.
%
Thus $\phi_c$ defines a family of path-imputation schemes, parameterised by a choice of data-imputation scheme.

Before we analyse \emph{why} this works in practice, we repeat a crucial
property of the signature transform~\citep[Appendix~A]{kidger2019deep}.
%
\begin{theorem}[Invariance to reparameterisation]\label{theorem:invariancetime}
  Let $f \colon [a, b] \to \reals^d$ be a continuous piecewise
  differentiable path. Let $\psi \colon [a, b] \to [c, d]$ be continuously
  differentiable, increasing, and surjective. Then $\sig(f) = \sig(f \circ
  \psi)$.
\end{theorem}
%
Coming back to our analysis, we first note that the previous theorem
implies that the signature transform of $\phi_c(\mathbf{x})$ is
invariant to the choice of $s_i^{(n)}$.
%
Second, note that holding time between observations fixed is a valid
choice, by the definition for $\seriesspace$ in equation
\eqref{eq:seriesspace}. There should hopefully be no moral objection to
our definition of $\seriesspace$, as holding time fixed essentially just
corresponds to a jump discontinuity; not such a strange thing to have
occur. Here, by replacing time as the parameterisation, we are then able
to recover the continuity of the path.
%
Third, we claim that  $\phi_c$ is immune to the two major flaws of
imputation methods, namely
%
\begin{inparaenum}[(i)]
  \item their fragile dependence on sampling in unrelated channels, and
  \item their non-causality.
\end{inparaenum}
%
Let us consider the first flaw of dependence on sampling in unrelated channels.
%
For simplicity, take $c$ to be the forward-fill data-imputation
strategy. Consider again the $\mathbf{x}$ defined in expression~\eqref{eq:flaw1}.
This means that
%
\begin{equation}\label{eq:causal1}
    \phi_c(\mathbf{x}) = \psi(\;((t_1, x_1^1, x_1^2), (t_2, x_1^1, x_1^2), (t_2, x_2^1, x_1^2))\;).
\end{equation}
%
Contrast adding in the extra observation at $t_{3/2}$ as in equation \eqref{eq:flaw2}. Then
%
\begin{align}
    &\phi_c(\mathbf{x})(s)\nonumber\\
    &=\psi(\;((t_1, x_1^1, x_1^2), (t_{3/2}, x_1^1, x_1^2), (t_{3/2}, x_1^1, x_{3/2}^2),\nonumber\\ &\hspace{3.1em}(t_2, x_1^1, x_{3/2}^2), (t_2, x_2^1, x_{3/2}^2))\;).\label{eq:causal2}
\end{align}
%
Evaluating each $\psi$ will then in each case give a path with three
channels, corresponding to $t, x^1, x^2$. Then it is clear that the $(t,
x^1)$ component of the path in equation~\eqref{eq:causal1} is just
a reparameterisation of the path in equation~\eqref{eq:causal2},
a difference which is irrelevant by Theorem~\ref{theorem:invariancetime}.
(And the $x^2$ component of the second
path has been updated to use the new information $x_{3/2}^2$.) Thus the causal path impuation scheme is robust to such issues. For general time series and
$c$ taken to be any other causal data-imputation strategy, then much the
same analysis can be easily be performed.

Now consider the second potential flaw, of non-causality. The issue
previously arose because of the non-causality of the linear
path-imputation. We see from equation \eqref{eq:causalsig}, however,
such changes only occur in data space while the time channel is frozen;
conversely the time channel only updates with the value in the data
space frozen. Provided that $c$ is also causal, then causality will,
overall, have been preserved. For example, it is possible to use this
scheme in an online setting.
%
There are interesting comparisons to be made between causal signature
imputation and certain operations in the signature literature. First is
the \emph{lead-lag} transform \cite{primer2016}. With the lead-lag
transform, the entire path is \emph{duplicated}, and then each side is
alternately updated. Conversely, in causal signature imputation, the
path is instead \emph{split} between $t$ and $(x^1, \ldots, x^n)$, and
then each side is alternately updated.
%
Second is the comparison to the linear and rectilinear embedding
strategies, see for example \cite{fermanian2019embedding}. It is
possible to interpret $\psi \circ \Phi$ as a hybrid between the linear
and rectilinear embeddings: it is rectilinear with respect to an
ordering of $t$ and $(x^1, \ldots, x^n)$, and linear on $(x^1, \ldots,
x^n)$.
Furthermore, the time-joined transformation \cite{levin2013} is pursuing a very similar goal to the here described causal signature imputation. This is also why we do not consider this imputation strategy as a novel contribution of this work.


\subsubsection{Comparison to the Fourier and wavelet transforms}\label{sec: comparison fourier}
The signature transform exhibits a certain similarity to the one-dimensional Fourier or wavelet transforms. Both are integrals of paths. However, in reality these transforms are fundamentally different. Both the Fourier and wavelet transforms are linear transforms, and operate on each channel of the input path separately. In doing so they model the path as a linear combination of elements from some basis.

Conversely, the signature transform is a nonlinear transform - indeed, it is a universal nonlinearity - and operates by combining information between different channels of the input path. In doing, the signature transform models \emph{functions of the path}; the universal nonlinearity property says that in some sense it provides a basis for such functions.

\end{document}



    

